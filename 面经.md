- java 对象的内存结构?标记字是做什么的?

- 👤

  continue

- 💡

  Java 对象的内存结构分为三部分：对象头、实例数据和对齐填充。

  对象头由两部分组成：Mark Word 和 Klass Pointer。Mark Word 是用来存储对象自身的运行时数据，如哈希码、GC 分代年龄以及锁状态等信息；Klass Pointer 则是指向该对象所属类的元数据指针。

  实例数据则是对象真正存储的数据，包括其定义的各个字段。

  对齐填充则是为了保证对象的起始地址是 8 字节的整数倍，从而提高访问速度。

  标记字属于对象头的一部分，是用来存储对象的状态信息，特别是在垃圾回收过程中非常重要。它可以记录对象是否被锁定、是否是偏向锁、是否需要进行标记清除等，从而帮助 JVM 更加高效地进行垃圾回收。
  
  
  
- 用Java写个单例？为何静态内部类实现的单例可以做到线程安全且可延迟加载？

- 💡

  好的，以下是Java中静态内部类实现单例模式的代码示例：

  ```
  public class Singleton {
      private Singleton() {}
  
      private static class SingletonHolder {
          private static final Singleton INSTANCE = new Singleton();
      }
  
      public static Singleton getInstance() {
          return SingletonHolder.INSTANCE;
      }
  }
  ```

  这个单例模式的实现方式被称为“静态内部类单例模式”。我们知道，Java的静态内部类可以访问外部类的 private 静态成员变量和方法。因此，在 SingletonHolder 内部类中声明并初始化 Singleton 类型的 INSTANCE 变量时，能够直接访问 Singleton 类的 private 构造方法。

  对于这个实现方式，为什么能够做到线程安全且可延迟加载呢？简单来说，是因为在类加载的过程中，静态内部类并不会被加载，只有在调用 Singleton.getInstance() 方法获取实例时才会首次加载 SingletonHolder 类，并创建 Singleton 类型的实例。并且，由于在类加载过程中，JVM 会获得类锁，因此能够保证该实例的初始化是线程安全的。

  相比之下，懒汉式单例模式（非线程安全）或者双重检验锁单例模式（需要使用 volatile 关键字保证线程安全）、枚举实现单例模式等都存在一些问题，而静态内部类单例模式则不仅简单，而且线程安全且能够延迟加载。



### new Hashmap<1000> 和 new Hashmap<10000> 在数据都塞满的时候有什么区别？（提示 扩容相关）

>  总结
> HashMap 默认容量是 16，默认的负载因子是 0.75f
> 构造方法只会初始化一个空的 Node 数组，真正的初始化过程是在 put 方法中的 putVal 里面调用的，主要核心方法是 resize 方法
> HashMap 的容量固定为 2n 次方，也就是如果我们传入了初始化容量，HashMap 不会使用我们传入的容量，而是会帮我们计算与我们传进去的参数最接近（大于等于）的一个 2n 的值作为容量
> HashMap 内部会存在一个阈值（threshold），该值为容量和负载因子的乘积，当 HashMap 里面的容量大于等于阈值时，会触发扩容。
> 因为扩容是一个拷贝数组的过程，比较耗费资源。建议在初始化的时候指定容量，才能最大限度的利用 HashMap 的性能。



面试题扩展
如果初始化传入容量为 1w，在存储第 1w 的键值对时，会触发扩容吗？

答：需要考虑两种情况，使用默认负载因子和自定义的负载因子

如果使用默认负载因子 0.75，初始化容量为 1w，那么 HashMap 帮我们生成的容量就是 214 次方为 16384，
根据默认负载因子计算出来的扩容阈值为 12288
也就是说，只有当 put 到第 12288 个元素的时候，才会触发 HashMap 的扩容，而存储第 1w 个元素时，还未到达阈值，不会触发扩容机制。
如果使用自定义的负载因子，那么就要根据具体情况来分析了，比如使用 0.5 作为负载因子，那么阈值计算出来就是 8192，显然会触发扩容
如果初始化传入容量为 1k 呢？会触发扩容吗？
答：根据上面的分析，按照默认负载因子来计算：初始化容量为 1k，HashMap 生成的容量就是 210 = 1024，阈值就是 1024 * 0.75 = 768，实际上不用 put 到 1000 个元素，put 到 768 个元素时就会触发扩容。
https://blog.csdn.net/lqw_student/article/details/108353039



- 👤

  java 弱引用和虚引用的区别？

  

- 💡

  Java中的弱引用和虚引用都是用于在内存不足时进行垃圾回收的。但它们之间有一些区别：

  1. 弱引用（Weak Reference）：当一个对象只被弱引用所引用时，垃圾回收器就会将该对象回收。在 Java 中，可以使用 java.lang.ref.WeakReference 类来创建弱引用。
  2. 虚引用（Phantom Reference）：虚引用是最弱的引用类型，它指向一个对象，但是不能通过虚引用来获取这个对象的实例。在 Java 中，可以使用 java.lang.ref.PhantomReference 类来创建虚引用。虚引用必须和一个引用队列（ReferenceQueue）一起使用，用来跟踪被回收的虚引用。

  主要区别：

  - 当一个对象只被弱引用所引用时，垃圾回收器就可以把该对象回收掉；当一个对象没有任何引用时，垃圾回收器才能回收该对象。
  - 虚引用指向的对象即使被回收了也无法通过虚引用访问到该对象，而弱引用仍然可以访问到被引用的对象。

  总之，弱引用和虚引用都属于 Java 中的引用类型，是为了更精细地控制垃圾回收而设计的。



- 👤

  垃圾回收时标记存活对象的三色标记法原理，以及在出现漏标、错标情况时是如何解决的？

  

- 💡

  三色标记法是一种常用于垃圾回收中的算法，其基本原理如下：

  首先，将堆中的所有对象都标记为白色（即未访问过）。然后，从根对象开始遍历整个对象图，将可达的对象标记为灰色（即已访问但还未处理），并将它们加入一个待处理队列中。接着，不断地从待处理队列中取出灰色对象，并将其直接引用的对象标记为灰色（如果该对象尚未被标记过）。当队列为空时，所有可达对象都已标记完毕，此时未被标记的对象即为无用对象，可以被回收。

  在此过程中，有三种颜色表示对象的状态：

  - 白色：对象未被访问过。
  - 灰色：对象已被访问过，但还有引用指向未被访问的对象。
  - 黑色：对象已被访问过，并且所有引用指向的对象都已经被访问过了。

  在标记阶段结束后，为了解决漏标或者错标的情况，需要进行“清除”和“压缩”两个步骤。

  清除阶段会遍历整个堆，将未被标记的对象全部回收，并将标记过的对象重新变为白色。

  压缩阶段则会将所有存活对象移动到一起，以便在后续的内存分配中获得更大的连续空间，提升程序的运行效率。

  总之，三色标记法是一种高效可靠的垃圾回收算法，但在实际使用时，为了保证其正确性，需要仔细处理漏标、错标等异常情况。





- 👤

  jvm 调优你如何做的？现象->排查过程->解决方式->不同解决方案的对比与选择

- 💡

  针对JVM调优，我通常会采取以下步骤：

  1. 观察现象：首先需要观察系统的运行状况，包括CPU利用率、内存使用情况、GC情况、线程数等指标。可以使用一些监控工具如jstat、jconsole、VisualVM等来帮助观察。
  2. 排查过程：根据观察到的现象，需要分析具体问题的原因。例如，如果观察到频繁的Full GC，那么就需要查看GC日志，分析哪些对象频繁地被回收、哪些地方可能存在内存泄漏等。
  3. 解决方式：根据排查过程中发现的问题，需要考虑使用什么方式来解决。例如，如果出现了内存泄漏的问题，可能需要进行代码修复；如果是内存使用过高导致的Full GC问题，可能需要调整堆大小、调整GC策略等。
  4. 不同解决方案的对比与选择：在确定多个解决方案后，需要进行选择和对比。这个过程需要考虑到不同方案的成本、风险和效果等因素。有时候需要根据实际情况，采取多种方式组合来解决问题。

  总之，JVM调优需要全面观察系统情况，深入分析具体问题，针对性选择解决方案，并且进行全面的对比与选择，才能达到最优效果。

- 👤

  多线程中的三大问题 java 是如何解决的？

  

- 💡

  Java 中多线程编程面临的三大问题是死锁、竞态条件和资源不足，针对这些问题，Java 提供了以下解决方案：

  1. 死锁：Java 提供了 `java.util.concurrent` 包中的锁机制，如 `ReentrantLock`、`ReadWriteLock` 和 `StampedLock` 等，它们可以替代传统的 synchronized 关键字，避免死锁问题的发生。
  2. 竞态条件：Java 提供了内置的同步机制，如 synchronized 关键字和 volatile 变量，通过控制对共享资源的访问来避免竞态条件问题的发生。
  3. 资源不足：Java 提供了线程池机制，通过预先创建一定数量的线程，可以提高线程的复用性和效率，并且避免创建过多线程导致系统资源不足的问题。

  除此之外，Java 还提供了一些其他的工具和框架，如 CountDownLatch、Semaphore、CyclicBarrier、Fork/Join 框架等，这些工具和框架可以帮助开发者更加方便地处理多线程编程中的问题。

- 👤

  Java 中的线程池ThreadPoolExecutor的参数一般设置为多少

- 💡

  线程池ThreadPoolExecutor的参数设置需要根据具体的场景和需求来定，不能一概而论。以下是几个常用的参数及其含义：

  1. corePoolSize：核心线程数，表示在池中保持的线程数，即使这些线程处于空闲状态也不会被销毁，除非设置了 allowCoreThreadTimeOut 选项。如果运行的任务数少于核心线程数，则池中的所有任务都将被放入任务队列中等待执行。
  2. maximumPoolSize：最大线程数，表示在池中允许的最大线程数。当任务队列已满且当前线程数小于最大线程数时，会创建新的线程执行任务。
  3. keepAliveTime：线程空闲时间，表示当线程处于空闲状态超过指定时间时，该线程将被销毁。该参数只有在 allowCoreThreadTimeOut 为 true 时才会生效。
  4. workQueue：任务队列，表示用于保存等待执行任务的阻塞队列。可以选择不同类型的队列，例如 SynchronousQueue、LinkedBlockingQueue 和 ArrayBlockingQueue 等，以满足不同的需求。
  5. threadFactory：线程工厂，用于创建新的线程。默认使用 Executors.defaultThreadFactory() 方法创建线程。

  在实际使用中，应根据具体情况选择合适的参数。例如，如果希望快速响应任务，可以将核心线程数设置得比较大，而最大线程数可以根据服务器性能和负载情况进行调整。如果任务量较大，可以使用有界的阻塞队列来限制任务数量，避免内存溢出。另外，还需要注意线程池的拒绝策略，以便在任务过多时合理处理。



- 👤

  synchronized 底层实现原理？释放锁之后如何通知其他线程获取锁？

  

- 💡

  `synchronized` 是 Java 中用于实现对象锁的关键字，用于保证多线程环境下共享数据的线程安全。底层实现原理是基于监视器锁（Monitor）实现的。

  当一个线程进入到 synchronized 代码块时，它会尝试获取对象锁。如果这个对象没被锁定，则该线程会获得对象锁，然后继续执行 synchronized 代码块。如果此时该对象已经被锁定，则该线程会进入阻塞状态，直到获得对象锁为止。

  释放锁的过程是在 synchronized 代码块执行结束或者遇到异常时自动释放的。在释放锁之后，jvm 会通知等待在对象监视器上的其他线程，这些线程竞争锁的机会被重新激活。竞争锁的线程会重新进入到执行 synchronized 代码块的等待队列中，然后尝试再次获取对象锁。

  需要注意的是，在 Java 中，只有使用 wait/notify/notifyAll 方法才能进行线程间的通信，可以在同步块或同步方法中使用这些方法来实现线程间的协调和通信。

- 👤

  讲讲 AQS?

- 💡

  AQS 是 AbstractQueuedSynchronizer 的缩写，是 Java 并发包中一个用于实现同步器的基础框架。AQS 提供了一种实现阻塞锁和相关同步器（如 Semaphore 和 CountDownLatch）的灵活且高效的方式。

  AQS 的核心思想是维护一个由 Node 节点构成的双向队列，这个队列中的节点表示获取锁或者其他同步状态的线程。AQS 维护了一个 volatile 的 state 变量，它表示同步状态的具体含义，在锁的情况下为 0 或者 1。当 state 为 1 时，表示有线程已经获取到了锁；当 state 为 0 时，则表示当前没有线程持有锁，可以尝试争夺锁。

  在 AQS 中，每个线程获取同步状态时，都会先通过 CAS 操作来尝试获取同步状态。如果 CAS 操作失败，线程会创建一个 Node 节点加入到队列的尾部，并阻塞自己。当锁释放时，会唤醒队头的节点，使其尝试获取同步状态。

  通过这种方式，AQS 实现了一种高效的线程同步机制，能够支持公平锁和非公平锁等不同种类的同步器。



- 👤

  Java中LongAdder 实现原理？

- 💡

  Java中的`LongAdder`是一个高效的线程安全累加器，它主要用于多线程环境下对long型整数进行累加操作。

  `LongAdder`的实现原理基于分离器（Separate）思想，即将原本的一个变量分离成多个变量，每个变量独立累加，最后汇总结果。这个思想可以减少多线程环境下的竞争和锁冲突，提高并发性能。

  `LongAdder`内部维护了一个`base`变量和一个`Cell`数组，`base`变量用于记录当前的累加值，而每个`Cell`对象都维护了一个`value`值，当多个线程同时进行累加操作时，`LongAdder`会根据线程的哈希值将累加操作分散到不同的`Cell`对象中，每个线程只需要对自己所分配的`Cell`对象进行累加操作，避免了线程之间的竞争和锁冲突。

  当需要获取累加结果时，`LongAdder`会将所有`Cell`对象的值加起来，并加上`base`变量的值，得出最终的累加结果。

  下面是`LongAdder`的简单示例：

  ```
  import java.util.concurrent.atomic.LongAdder;
  
  public class LongAdderDemo {
      public static void main(String[] args) {
          LongAdder adder = new LongAdder();
  
          for (int i = 0; i < 10; i++) {
              new Thread(() -> {
                  for (int j = 0; j < 10000; j++) {
                      adder.increment();
                  }
              }).start();
          }
  
          try {
              Thread.sleep(1000);
          } catch (InterruptedException e) {
              e.printStackTrace();
          }
  
          System.out.println(adder.sum());
      }
  }
  ```

  在上面的示例中，我们创建了10个线程，每个线程累加10000次，最后输出`LongAdder`的累加结果。由于`LongAdder`是线程安全的，所以多个线程可以同时对其进行累加操作，从而提高了程序的并发性能。



- 👤

  Java中动态代理的实现方式有哪些？对比与选择？

- 💡

  Java 中动态代理的实现方式有两种：基于接口的动态代理和基于类的动态代理。

  基于接口的动态代理使用 Java 内置的 InvocationHandler 接口，实现动态代理只需要为一个或多个接口创建一个 InvocationHandler 实例，然后通过 Proxy 类的静态方法 newProxyInstance() 创建一个代理对象，代理对象会调用 InvocationHandler 的方法来执行被代理接口的方法。这种方式的局限是只能代理实现了接口的类。

  基于类的动态代理使用 CGLIB 库，它可以代理没有实现接口的类，通过继承被代理类并重写其方法以实现代理功能。CGLIB 库通过在运行时生成被代理类的子类来实现代理，然后将代理方法的调用委托给 InvocationHandler 实例。

  选择哪种方式要根据具体需求而定。如果被代理类实现了接口，则使用基于接口的动态代理；如果被代理类没有实现接口，或者需要代理类的非公共方法，则使用基于类的动态代理。但是CGLIB需要引入第三方库，代理速度也比基于接口的动态代理稍慢一些。



- 👤

  MySQL中的索引为何使用 b+树而不是使用别的数据结构？ 索引下推？倒排索引？

- 💡

  MySQL中使用B+树作为索引的数据结构主要有以下几个原因：

  1. 支持高效的范围查询：B+树的叶子节点是按照键值排列的，因此可以通过在非叶子节点进行二分查找来快速定位需要查询的范围，并且可以直接遍历相邻的叶子节点来获取整个范围内的数据。
  2. 支持顺序访问：由于B+树的叶子节点按照键值排序，因此可以直接从叶子节点开始顺序读取数据，从而支持快速的顺序访问。
  3. 支持高效的插入和删除操作：B+树的插入和删除操作相对于其他数据结构来说比较高效，因为B+树只涉及到局部的节点调整，而不需要重建整个索引。

  索引下推（Index Condition Pushdown）是MySQL优化器针对查询优化的一种技术，它可以将查询条件下推到存储引擎层执行，减少了从存储引擎中读取数据的次数，从而提高了查询性能。

  倒排索引可以快速地定位包含某个关键词的记录，是用于全文索引的一种数据结构，而B+树主要用于范围查询和等值查询。虽然倒排索引在全文搜索等场景下非常优秀，但是在MySQL中，B+树对于支持范围查询和排序等操作来说更加适合



- 
  讲讲MySQL的事务，ACID 隔离级别，幻读如何出现的 又是如何解决？)

- 💡

  MySQL 的事务是指一组逻辑上的操作，这些操作要么全部成功执行，要么全部回滚。MySQL 支持 ACID（原子性、一致性、隔离性和持久性）属性，确保数据库操作的可靠性。

  ACID 属性：

  - 原子性（Atomicity）：事务的所有操作被看作单个操作，要么全部执行成功，要么全部回滚。
  - 一致性（Consistency）：事务开始前和结束后，数据库都必须处于一致性状态。
  - 隔离性（Isolation）： 多个事务同时执行时相互之间是隔离的，一个事务的执行不应该受到其他事务的干扰。
  - 持久性（Durability）：已经提交的事务对数据库的修改必须永久保存到数据库中。

  MySQL提供了四种隔离级别，从低到高依次为： READ UNCOMMITTED、READ COMMITTED、REPEATABLE READ 和 SERIALIZABLE

  - READ UNCOMMITTED（读未提交）：允许一个事务读取另一个未提交的事务所做的变更，可能造成脏读，数据不一致。
  - READ COMMITTED（读已提交）：只允许一个事务读取另一个已提交的事务所做的变更，避免了脏读，但是会造成不可重复读（指在同一事务中读取同一行两次，发现两次结果不同）。
  - REPEATABLE READ（可重复读）：保证在同一个事务中多次读取同一数据时，结果是一致的。但是可能会导致幻读问题（指一个事务先后两次查询同一范围的数据，发现数据条数不同）。
  - SERIALIZABLE（串行化）：所有事务串行执行，效率最低，但是可以避免脏读、不可重复读和幻读。

  幻读是指一个事务在读取某个范围内的记录时，另一个事务在该范围内插入了新的记录，从而导致前一个事务再次读取该范围时，出现之前未出现过的行，从而产生了幻觉。

  为了解决幻读问题，MySQL采用了MVCC（多版本并发控制）机制，即为每个事务创建一个快照，在读取时只读取快照中的数据，即使其他事务对数据进行了修改，也不会影响当前读取的结果。在更新时，MySQL会将旧数据标记为已删除，并创建新的数据版本，从而实现事务的隔离性。



- 👤

  MySQL在不同隔离级别下是如何加锁的？

  

- 💡

  MySQL在不同的隔离级别下使用了不同的锁机制来保证事务的隔离性。以下是MySQL在不同隔离级别下的锁机制：

  1. READ UNCOMMITTED（读未提交）：在此隔离级别下，MySQL不会对读取的数据加锁，因此可能会发生脏读。
  2. READ COMMITTED（读已提交）：在此隔离级别下，MySQL只会对要修改的行及时加锁，读取的行不会被锁定。如果两个事务同时修改同一行，后执行的事务需要等待前一个事务结束后才能获得锁进行修改，避免了脏读问题。
  3. REPEATABLE READ（可重复读）：在此隔离级别下，MySQL会对查询结果集中的所有行都加共享锁，即SELECT操作返回的数据集中的每一条记录都加锁，避免了不可重复读问题。在UPDATE、DELETE、INSERT等操作时，MySQL会对修改的行加排他锁，从而保证其他事务不能修改该行。
  4. SERIALIZABLE（串行化）：在此隔离级别下，MySQL会对查询结果集中的所有行都加共享锁，并将这些锁持续到事务结束，以保证事务的串行执行。在UPDATE、DELETE、INSERT等操作时，MySQL会对整张表加排他锁，其他事务不能同时修改该表的任何行。

  总的来说，MySQL在不同隔离级别下的锁机制都是为了保证事务的隔离性，只不过在锁的粒度、范围和时间等方面有所不同。需要注意的是，在高并发的场景下，过多的锁会导致系统性能下降，因此需要在保证数据一致性的情况下尽量减少锁的使用。





- 👤

  mvcc 机制(实现原理以及 rr 和 rc 隔离级别下实现的区别？)

- 💡

  MVCC (Multi-Version Concurrency Control) 是一种多版本并发控制机制，用于解决数据库并发访问时出现的幻读、不可重复读等问题。MySQL 中就是通过使用 MVCC 来实现事务的隔离性。

  在MVCC机制中，每个事务在开始时会创建一个视图(View)，它包含了此时数据库中所有已提交的数据。而每次查询时，都是基于这个事务创建的视图(View)来执行的。每个事务都只能看到在自己开始之前已经提交的数据，而对于其他未提交的数据，则是不可见的。

  MVCC 通过在数据行中保存多个版本来实现，包括了旧的版本和新的版本。在更新操作时，MySQL 会将旧版本的数据标记为删除，同时创建新版本的数据，在查询时会根据版本号来筛选出需要的数据版本。

  在 REPEATABLE READ 隔离级别下，MySQL 基于MVCC机制创建了一个一致性读视图，即所有事务看到的数据都是一致的。在执行SELECT语句时，MySQL会将该事务启动时所有已经提交的事务版本都加入到该事务的一致性读视图中，然后在基于这些版本号构建的数据快照中进行读取操作，保证了读取的数据都是一致的。

  而在 READ COMMITTED 隔离级别下，MySQL仍然使用MVCC机制，但是每个SELECT语句都会为该事务创建一个新的一致性读视图，而不是像 REPEATABLE READ 隔离级别下一样在启动时就创建。因此，即使其他事务在此期间进行了提交，也不会影响之前创建的一致性读视图中的数据。所以，RC隔离级别下，一个事务可以读到其他事务已经提交的新插入的行，或者更改的行。

  需要注意的是，在MVCC机制下，对于 UPDATE、DELETE 和 INSERT 操作，MySQL 会为其每个版本都创建一个 undo 日志，用于回滚操作。这些 undo 日志会占用大量磁盘空间，因此要定期清理。



- 👤

  redolog undolog binlog(分别是用来做什么的 有什么共同点 区别？)

- 💡

  在 MySQL 中，Redo Log、Undo Log 和 Binlog 都是用来记录修改操作的日志，但是它们所记录的内容和作用不同。

  1. Redo Log：是一种用于崩溃恢复的日志，用于记录每个事务所执行的修改操作。它采用顺序写的方式记录，每次写入都是追加到 Redo Log 的末尾，并且 Redo Log 是循环写入的，也就是说，当 Redo Log 满了之后，旧的记录会被覆盖掉。Redo Log 在数据库启动时会被加载到内存中，以提高崩溃恢复的速度。
  2. Undo Log：是MySQL用于回滚事务和实现 MVCC 的一种日志，用于记录每个事务所执行的修改操作的反向操作。Undo Log 用于撤销修改并回滚事务，保证事务的原子性和一致性。与 Redo Log 不同的是，Undo Log 是支持随机读写的，即可以从文件的任意位置读取或写入数据。在MySQL中，Undo Log默认使用InnoDB存储引擎实现，并且存储在共享表空间中。
  3. Binlog：是一种用于记录数据库所有修改操作的日志，包括 DDL 和 DML 操作，而 Redo Log 和 Undo Log 只记录 DML。Binlog 是MySQL自带的一种二进制日志文件，它是从 MySQL 5.0 版本引入的。Binlog 记录了每个修改操作及其执行时间、执行用户等信息，可以用于数据库备份、复制和恢复操作。

  共同点：

  - 都是MySQL中用于记录修改操作的日志。
  - 都支持对事务进行回滚操作。

  区别：

  - Redo Log 和 Undo Log 记录了每个事务所执行的修改操作，而 Binlog 则记录了所有修改操作。
  - Redo Log 记录的是物理日志，Undo Log 记录的是逻辑日志，而 Binlog 记录的是逻辑日志。
  - Redo Log 和 Binlog 的写入方式都是顺序写，而 Undo Log 支持随机读写。
  - Redo Log 用于崩溃恢复，Undo Log 用于回滚操作和 MVCC 实现，Binlog 用于备份、复制和恢复操作。

- 👤

  Redis底层数据结构有哪些？跳表实现原理？为何不用红黑树？

- 💡

  Redis底层数据结构有以下几种：

  1. 字符串（string）
  2. 列表（list）
  3. 集合（set）
  4. 有序集合（sorted set）
  5. 哈希表（hash）
  6. Bitmaps
  7. HyperLogLog

  Redis使用跳表（skip list）来实现有序集合(sorted set)，跳表是一种基于链表的数据结构，可以实现快速的查找、插入和删除操作。跳表通过在链表中加入多级索引来加速查询，同时保持了链表的灵活性。

  相对于红黑树，跳表的实现更为简单，并且在实际使用中比红黑树的性能更好。由于红黑树的实现比较复杂，而Redis需要在内存中维护数据结构，因此选择了跳表作为有序集合的底层实现。





- 👤

  讲一讲Redis 的持久化

- 💡

  Redis支持两种持久化方式：RDB持久化和AOF持久化。

  RDB持久化是指将Redis在内存中的数据定期写入磁盘，生成快照文件(.rdb)，用于数据备份和恢复。我们可以通过设置save或者bgsave参数来触发RDB持久化。其中，save参数是指当在给定时间间隔内有N个键被修改时，自动保存快照文件；而bgsave参数则是异步进行保存，不阻塞Redis客户端的请求处理进程。

  相比AOF持久化，RDB持久化的优势在于恢复速度更快、生成的快照文件较小。但其也存在一些缺陷，比如在RDB持久化过程中若系统宕机，则会导致最近一次快照生成后的所有数据丢失。

  AOF持久化则是将Redis执行的每条写命令追加到日志文件中，以达到持久化的目的。我们可以通过设置appendonly参数来开启AOF持久化功能。AOF持久化具有更好的数据保护能力，但在数据规模较大时，AOF文件比RDB文件更大，并且AOF文件会随着操作日志的增长而增长，可能会影响性能。为此，Redis提供了rewrite功能，可以周期性地通过重写AOF文件来解决此问题。

  在实际应用中，我们可以根据数据的安全级别和性能要求，选择不同的持久化方式。同时，也可以将两种持久化方式配合使用，以提高数据的持久化能力和恢复速度。

  

  开销: 0.01824 点数

- 👤

  Redis 主从、哨兵、集群工作原理？三种部署方式的区别？

- 💡

  Redis提供了三种不同的复制方式，包括主从复制、哨兵模式和集群模式。

  1. 主从复制：

  在主从复制模式中，存在一个Redis主节点和多个Redis从节点。主节点将所有写操作记录到自己的AOF或RDB文件中，并将写操作以指令的形式发送给所有从节点。从节点接收到指令后，在自己的内存中执行相应的操作，以达到数据一致性的目的。在主节点宕机时，可以手动指定某一从节点为新的主节点，使系统继续正常工作。

  1. 哨兵模式：

  在哨兵模式中，存在多个Redis节点，其中一个节点作为主节点，其他节点作为从节点。同时，还会有一个专门的哨兵进程监控主节点的状态，发现主节点宕机后，会自动将一个从节点晋升为新的主节点，以保证系统的可用性。哨兵模式需要至少3个节点来保证高可用性，通常建议使用奇数个节点来避免网络分区问题。

  1. 集群模式：

  在集群模式中，多个Redis节点之间没有明确的主从关系，每个节点都是对等的。在集群中，又将所有数据分成多个槽(slot)，每个节点负责处理其中的一部分槽，以达到负载均衡的目的。客户端可以通过集群客户端API，直接访问集群中的任一节点完成操作。

  三种部署方式的区别：

  - 主从复制模式相对比较简单，适用于读多写少的场景。
  - 哨兵模式相对于主从复制，增加了自动故障转移和自动恢复等功能，适用于对高可用性有要求的场景。
  - 集群模式则适用于需要Redis支持大规模数据处理和高并发读写操作的场景，相比于主从复制和哨兵模式，对数据水平扩展和读写性能都做了很好的优化。

  开销: 0.03072 点数

- 👤

  缓存穿透、击穿、打满、雪崩出现的原因与常用解决办法

- 💡

  缓存穿透、击穿、打满和雪崩是常见的缓存问题，它们的出现原因和解决方案如下：

  1. 缓存穿透

  缓存穿透指的是大量请求查询不存在于缓存中的数据，导致这些请求都直

  开销: 0.03338 点数

- 👤

  continue

- 💡

  接着落在数据库上，使数据库负载过重，甚至会导致数据库宕机。出现缓存穿透的主要原因是恶意攻击或者程序错误等。

  解决办法：

  - 在应用层增加校验，比如对请求参数的合法性进行判断。
  - 在缓存层设置布隆过滤器等技术，用于判断请求的key是否有效，从而避免无效请求打到数据库上。

  1. 缓存击穿

  缓存击穿指的是一个存在于缓存中的数据，在发生并发请求时，有一些请求查询到的数据不存在于缓存中，导致这些请求都直接落在数据库上，使数据库负载过重。出现缓存击穿的主要原因是缓存中的数据失效或者被清除了。

  解决办法：

  - 对于热点数据，采用永不过期的方式，以保证数据能够一直存在于缓存中。
  - 对于过期的数据，采用互斥锁等方式，保证只有一个线程重新加载数据，并将数据更新到缓存中。

  1. 缓存打满

  缓存打满指的是缓存中存在大量的数据，导致内存空间被占满，而新的请求无法找到可用的内存空间，导致请求直接落在数据库上。出现缓存打满的主要原因是应用程序中存在的内存泄漏或者缓存空间配置不合理。

  解决办法：

  - 对于内存泄漏，需要通过代码分析等方式来找到并解决问题。
  - 根据业务情况设置合适的缓存策略，比如自动清除过期数据、按照LRU等算法清除长时间未使用的数据等方式，以用有限的内存空间存储尽可能多的热点数据。

  1. 缓存雪崩

  缓存雪崩指的是因为缓存中大量的数据过期失效，导致新的请求都无法从缓存中获取数据，同时请求量过大，直接落在数据库上，使数据库负载过重，甚至发生宕机。出现缓存雪崩的主要原因是缓存中数据失效时间设置不合理或者应用程序在某一时刻发生异常。

  解决办法：

  - 设置合理的缓存策略，避免大量数据同时失效。
  - 在发生雪崩时，可以采用分批重启服务、降低请求对数据库的压力等方式，先保证服务的可用性，再逐步恢复缓存中的数据。

  开销: 0.04864 点数

- 👤

  Redis中热 key 的解决方案（如何发现 如何优化）

- 💡

  热 key 问题是指某一个 key 的访问频率非常高，导致系统的性能和稳定性受到影响。解决方案主要有两种：发现热 key 和优化热 key。

  1. 发现热 key

  发现热 key 可以通过 Redis 提供的一些命令和工具实现，包括以下：

  - INFO 命令可以查看当前 Redis 中所有 key 的信息，如访问时间、过期时间等。
  - MONITOR 命令可以监视所有 Redis 客户端执行的命令，从而实时获取 key 的访问情况。
  - Redis 提供了一些监控工具，如 redis-cli、redis-stat、RedisLive 等工具可用于监视 Redis 的性能指标和 key 的使用情况。

  1. 优化热 key

  优化热 key 可以采用以下方式：

  - 对于读多写少的 key，可以使用 Redis 的缓存功能，将这些 key 存储到内存中，减少对数据库的访问。
  - 对于写多读少的 key，可以采用 Redis 的缓存更新机制，即先将数据更新到 Redis 中，再异步更新到数据库中，并适当调整 Redis 的数据同步策略，以提高系统性能。
  - 对于访问频率最高的 key，可以采用 Redis 集群或者分片技术，将数据分散到多台机器上，从而缓解单个机器的负载压力。
  - 可以采用 Redis 内置的 LRU 策略或者手动清理 Redis 中不常用的 key，以释放更多的内存空间。

  总之，要想解决热 key 问题，关键是要先了解 key 的使用情况，通过合适的技术手段进行优化，最终达到提高系统性能和稳定性的效果。



- 👤

  1、Kafka 基本工作原理？

  

- 💡

  Kafka 是一种高吞吐量、分布式的发布订阅消息系统，其基本工作原理如下：

  1. Kafka 的消息以 topic 的形式进行组织，每个 topic 包含多个分区（partition）；
  2. 消息发送者称为 producer，将消息发送到指定的 topic 中；
  3. Kafka 集群中的 broker 负责存储和分发消息，每个 broker 可以管理多个分区；
  4. 消息消费者称为 consumer，从指定 topic 的分区中读取消息；
  5. 为了实现高可用性和可伸缩性，Kafka 使用了副本机制来保证数据的可靠性，每个分区都有一个或多个副本；
  6. Kafka 的消息存储在磁盘上，因此可以支持大规模的消息存储和处理。

  总之，Kafka 的基本工作原理是通过将消息按照主题进行分区存储，并使用分布式集群来协调消息的处理和传输，从而实现高吞吐量、高可用性和可伸缩性的消息服务。

  开销: 0.01216 点数

- 👤

  Kafka 为何高吞吐？

- 💡

  Kafka 之所以能够实现高吞吐量，主要有以下几个方面的原因：

  1. 磁盘存储：Kafka 将消息存储在磁盘上，而不是内存中，这使得 Kafka 能够处理大量的消息，而不会受限于可用内存的大小。
  2. 批处理：Kafka 的 producer 可以将多条消息批量发送给 broker，从而避免了频繁的网络开销，提高了系统的吞吐量。
  3. 零拷贝技术：Kafka 使用零拷贝技术来避免在网络传输过程中对数据进行重复的序列化和反序列化操作，从而进一步提高了数据的处理效率。
  4. 分区机制：Kafka 对消息进行分区存储，并且允许多个 consumer 并行读取同一个分区，从而实现了并发处理，进一步提高了吞吐量。
  5. 副本机制：Kafka 采用副本机制保证数据的可靠性，并且允许 consumer 从副本中读取数据，从而进一步提高了数据的可靠性和可用性。

  综上所述，Kafka 通过多种技术手段提高了系统的性能和可靠性，从而实现了高吞吐量的消息处理。

  开销: 0.01734 点数

- 👤

  Kafka 消息的可靠性、顺序性是如何实现的的？

- 💡

  Kafka 实现消息的可靠性和顺序性，主要依靠以下两个机制：

  1. 副本机制：Kafka 采用副本机制来保证消息的可靠性。每个分区都会有一个或多个副本，其中一个副本称为 leader，其余副本称为 follower。producer 发送的消息首先会写入 leader 副本中，并等待 leader 将消息同步到所有 follower 副本后再返回确认信息给 producer。这样无论 leader 或 follower 副本出现故障，都可以从其他副本复制数据来确保数据不会丢失。
  2. 分区机制：Kafka 对消息进行分区存储，并允许 producer 将消息发送到指定的分区中。在同一个分区中，消息的顺序是有序的。因此，如果 producer 将某个 topic 的消息发送到同一个分区中，那么 Kafka 可以保证这些消息被消费时的顺序是一致的。但是，在多个分区中的消息可能就没有顺序保证了。

  总之，Kafka 通过副本机制保障了消息的可靠性，通过分区机制保障了同一分区内消息的顺序性。这些机制使得 Kafka 能够提供高可靠性、高吞吐量的消息传递服务。

  开销: 0.0253 点数

- 👤

  Kafka 的 ISR 机制？

- 💡

  ISR 指的是 Kafka 中的 in-sync replicas，即与 leader 副本保持同步的副本集合。ISR 机制是 Kafka 中保证副本之间数据一致性的机制，主要有以下两个作用：

  1. 确保 leader 副本不会因为某些原因导致数据丢失。当 leader 副本出现故障时，Kafka 会自动将一个 follower 副本升级为新的 leader，并在新 leader 写入数据前等待所有与旧 leader 同步的 follower 副本都将数据同步到位。
  2. 提高读取性能。Kafka 允许 consumer 从 follower 副本中读取消息，这样可以减轻 leader 副本的压力，提高整个集群的读取性能。

  ISR 机制的实现方式如下：

  1. 每个分区都有一个 LEO（Log End Offset），即分区中最后一条消息的偏移量。Kafka 记录每个副本的 LEO，并维护一个 ISR 集合，该集合包含了所有与 leader 副本的 LEO 差距在一定范围内的 follower 副本。
  2. 当 follower 副本无法及时地将数据同步到 leader 副本时，它的 LEO 就会落后于 leader 的 LEO，这时 Kafka 会将该 follower 副本从 ISR 集合中移除。
  3. 如果某个 follower 副本在一段时间内无法与 leader 副本同步数据，它将被认为已经失效，并从 ISR 集合中移除，直到它重新追上 leader 副本的 LEO。

  ISR 机制通过保持数据同步，确保了 Kafka 的高可靠性和高吞吐量。

  开销: 0.03526 点数

- 👤

  Kafka 与其他 MQ 的对比与选择

- 💡

  Kafka 是一种分布式、高吞吐量的消息队列，与其他消息队列相比，它具有以下几个优点：

  1. 高吞吐量：Kafka 的磁盘存储、批处理和分区机制等特性使得其能够实现非常高的吞吐量。
  2. 可靠性：Kafka 的副本机制和 ISR 机制保证了数据的可靠性和一致性。
  3. 可扩展性：Kafka 的分布式架构允许用户根据需要增加或减少节点数量，从而实现横向扩展。
  4. 多语言支持：Kafka 支持多种编程语言，如 Java、Python、Node.js 等。
  5. 开源免费：Kafka 是一款开源软件，可以免费使用和修改。

  当然，Kafka 也存在一些限制和缺点，例如：

  1. 实时性较差：Kafka 无法保证消息的实时性，因为它必须将消息先写入磁盘再进行处理。
  2. 不支持事务：Kafka 目前不支持跨分区的事务，这意味着如果需要在多个分区之间执行原子操作，则需要实现自己的解决方案。
  3. API 较为复杂：Kafka 的 API 相对于其他消息队列可能会更加复杂，需要用户具备一定的编程经验。

  除了 Kafka 之外，市场上还存在许多其他的消息队列，例如 RabbitMQ、ActiveMQ、RocketMQ 等。这些消息队列各有优劣，用户可以根据自己的需求和技术水平选择最适合自己的产品。